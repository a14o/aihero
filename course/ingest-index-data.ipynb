{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "448ecd0c-a588-4d12-bee7-d9a6e7dee668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62535ae0-f4ce-4a2a-bb38-85bf58c657f0",
   "metadata": {},
   "source": [
    "# Ingest and Index Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e146cc18-075c-4459-af9a-09c65799f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764ba806-8d30-45b2-aca4-fa1d44f1fde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1217\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f583a096-1160-4db2-838a-a539eb05d6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: docs-main/examples/LLM_regression_testing.mdx\n",
      "Metadata: {}\n",
      "Content preview:\n",
      "In this tutorial, you will learn how to perform regression testing for LLM outputs.\n",
      "\n",
      "You can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\n",
      "\n",
      "<Info>\n",
      "  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\n",
      "</Info>\n",
      "\n",
      "# Tutorial scope\n",
      "\n",
      "Here's what we'll do:\n",
      "\n",
      "* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\n",
      "\n",
      "* **Get new answers**. Imitate generating new answers to the same question.\n",
      "\n",
      "* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\n",
      "\n",
      "* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time...\n"
     ]
    }
   ],
   "source": [
    "# Previews Evidently document at index 45\n",
    "evidently_doc = evidently_docs[45]\n",
    "print(f\"Filename: {evidently_doc['filename']}\")\n",
    "print(f\"Metadata: {evidently_doc.get('attributes', {})}\")\n",
    "print(f\"Content preview:\\n{evidently_doc['content'][:1000]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa66dd-d551-4af6-8f1c-7e9fed02fac2",
   "metadata": {},
   "source": [
    "# Simple Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a60d30ba-a9c7-40dc-ad9a-9f4080225818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "589ab8a5-e8b2-4c2f-85ff-519b4140ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c0510a9-3305-4932-841c-bf67880f85f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 0 ---\n",
      "Filename: docs-main/api-reference/introduction.mdx\n",
      "Start Index: 0\n",
      "Content Preview:\n",
      "<Note>\n",
      "  If you're not looking to build API reference documentation, you can delete\n",
      "  this section by removing the api-reference folder.\n",
      "</Note>\n",
      "\n",
      "## Welcome\n",
      "\n",
      "There are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\n",
      "\n",
      "<Card\n",
      "  title=\"Plant Store Endpoints\"\n",
      "  icon=\"leaf\"\n",
      "  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\n",
      ">\n",
      "  View the OpenAPI specification file\n",
      "</Card>\n",
      "\n",
      "## Authentication\n",
      "\n",
      "All API endpoints are authenticated using Bearer tokens and picked up from the specification file.\n",
      "\n",
      "```json\n",
      "\"security\": [\n",
      "  {\n",
      "    \"bearerAuth\": []\n",
      "  }\n",
      "]\n",
      "```...\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Filename: docs-main/changelog/changelog.mdx\n",
      "Start Index: 0\n",
      "Content Preview:\n",
      "<Update label=\"2025-07-18\" description=\"Evidently v0.7.11\">\n",
      "  ## **Evidently 0.7.11**\n",
      "\n",
      "  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.11).\n",
      "\n",
      "Example notebooks:\n",
      "- Synthetic data generation: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)\n",
      "\n",
      "</Update>\n",
      "\n",
      "<Update label=\"2025-07-09\" description=\"Evidently v0.7.10\">\n",
      "  ## **Evidently 0.7.10**\n",
      "    Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.10).\n",
      "  \n",
      "NEW: automated prompt optimization. Read the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\n",
      "\n",
      "Example notebooks:\n",
      "- Code review binary LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\n",
      "- Topic multi-class LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evide...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Filename: docs-main/changelog/changelog.mdx\n",
      "Start Index: 1000\n",
      "Content Preview:\n",
      "ntly/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\n",
      "- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)\n",
      "</Update>\n",
      "\n",
      "<Update label=\"2025-06-27\" description=\"Evidently v0.7.9\">\n",
      "  ## **Evidently 0.7.9**\n",
      "\n",
      "  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\n",
      "</Update>\n",
      "\n",
      "<Update label=\"2025-06-19\" description=\"Evidently v0.7.8\">\n",
      "  ## **Evidently 0.7.8**\n",
      "\n",
      "  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\n",
      "</Update>\n",
      "\n",
      "<Update label=\"2025-06-04\" description=\"Evidently v0.7.7\">\n",
      "  ## **Evidently 0.7.7**\n",
      "\n",
      "  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\n",
      "</Update>\n",
      "\n",
      "<Update label=\"2025-05-25\" description=\"Evidently v0.7.6\">\n",
      "  ## **Evidently 0.7.6**\n",
      "\n",
      "  Full release notes on [Github](https://github.com/evidentlyai/evidently/r...\n"
     ]
    }
   ],
   "source": [
    "# Preview first 3 chunks\n",
    "for i, chunk in enumerate(evidently_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(f\"Filename: {chunk.get('filename')}\")\n",
    "    print(f\"Start Index: {chunk['start']}\")\n",
    "    print(f\"Content Preview:\\n{chunk['chunk'][:1000]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0155ce-95ff-431f-9e33-667068177635",
   "metadata": {},
   "source": [
    "# Split by Paragraphs and Section"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7570ad27-7fe1-4eac-b508-bcee558c216a",
   "metadata": {},
   "source": [
    "# For simple document files\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46542a13-07e1-412a-a8cf-ac98f5d2bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8e2039f-1646-42b6-81dc-8d7270dab17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = split_markdown_by_level(text, level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60e89565-8e26-4784-988a-e4e826d23ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c37295c-6068-4712-a174-4f4fb301aaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sections (chunks) extracted: 262\n",
      "\n",
      "--- Chunk 0 ---\n",
      "Filename: docs-main/api-reference/introduction.mdx\n",
      "Metadata: {}\n",
      "Section Preview:\n",
      "## Welcome\n",
      "\n",
      "There are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\n",
      "\n",
      "<Card\n",
      "  title=\"Plant Store Endpoints\"\n",
      "  icon=\"leaf\"\n",
      "  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\n",
      ">\n",
      "  View the OpenAPI specification file\n",
      "</Card>...\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Filename: docs-main/api-reference/introduction.mdx\n",
      "Metadata: {}\n",
      "Section Preview:\n",
      "## Authentication\n",
      "\n",
      "All API endpoints are authenticated using Bearer tokens and picked up from the specification file.\n",
      "\n",
      "```json\n",
      "\"security\": [\n",
      "  {\n",
      "    \"bearerAuth\": []\n",
      "  }\n",
      "]\n",
      "```...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Filename: docs-main/docs/library/data_definition.mdx\n",
      "Metadata: {}\n",
      "Section Preview:\n",
      "## Basic flow\n",
      "\n",
      "**Step 1. Imports.** Import the following modules:\n",
      "\n",
      "```python\n",
      "from evidently import Dataset\n",
      "from evidently import DataDefinition\n",
      "```\n",
      "\n",
      "**Step 2. Prepare your data.** Use a pandas.DataFrame.\n",
      "\n",
      "<Info>\n",
      "  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\n",
      "</Info>\n",
      "\n",
      "**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\n",
      "\n",
      "```python\n",
      "eval_data = Dataset.from_pandas(\n",
      "    source_df,\n",
      "    data_definition=DataDefinition()\n",
      ")\n",
      "```\n",
      "\n",
      "To map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\n",
      "\n",
      "- By type (numerical, categorical).\n",
      "- By matching column names to roles (e.g., a column \"target\" treated as target).\n",
      "\n",
      "Automation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Preview first 3 chunks\n",
    "print(f\"Total sections (chunks) extracted: {len(evidently_chunks)}\")\n",
    "for i, chunk in enumerate(evidently_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(f\"Filename: {chunk.get('filename')}\")\n",
    "    print(f\"Metadata: {chunk.get('attributes', {})}\")\n",
    "    print(f\"Section Preview:\\n{chunk['section'][:1000]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1db34-1a3c-4363-a70a-f37492023a47",
   "metadata": {},
   "source": [
    "# Intelligent Chunking with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31180f96-501c-4fd4-ae2b-823f0a7eee82",
   "metadata": {},
   "source": [
    "Models used:\n",
    "- x-ai/grok-4-fast:free\n",
    "- deepseek/deepseek-chat-v3.1:free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fb8aab7-fbcb-4777-b5f4-fd4359d69e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'gen-1758891663-OISN7rPd8HbsGGSlXKrR', 'provider': 'DeepInfra', 'model': 'deepseek/deepseek-chat-v3.1:free', 'object': 'chat.completion', 'created': 1758891663, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'native_finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': 'That is one of the most profound and enduring questions humanity has ever asked. There isn\\'t one single, definitive answer because the meaning of life is deeply personal and often philosophical. Different perspectives offer various ways to think about it.\\n\\nHere’s a breakdown of how different fields and philosophies approach the question:\\n\\n### 1. Philosophical Perspectives\\n\\n*   **Existentialism (e.g., Sartre, Camus):** This philosophy argues that life has no *inherent* meaning (\"the universe is indifferent\"). Therefore, it is up to each individual to create their own meaning and purpose through their actions, choices, and passions. As Jean-Paul Sartre said, \"Man is condemned to be free... because once thrown into the world, he is responsible for everything he does.\"\\n*   **Absurdism (Camus):** Closely related, Albert Camus argued that humans have a natural desire to find meaning, but the universe provides none. This conflict is \"the absurd.\" The solution is not suicide or despair, but to rebel against the absurd by embracing life and finding your own purpose, all while understanding its ultimate meaninglessness. He famously said, \"The meaning of life is whatever keeps you from killing yourself.\"\\n*   **Stoicism:** Meaning is found not in chasing pleasure or avoiding pain, but in living a life of virtue, reason, and duty. It\\'s about focusing on what you can control (your actions, your responses) and accepting what you cannot control (external events, the actions of others).\\n*   **Nihilism:** This is the belief that life has no intrinsic meaning, value, or purpose. While often seen as pessimistic, it can also be liberating—if nothing matters, you are free to define what matters to *you*.\\n\\n### 2. Religious & Spiritual Perspectives\\n\\nMost religions propose that meaning is provided by a higher power or a divine plan.\\n\\n*   **Theistic Religions (Christianity, Islam, Judaism):** Meaning is found in a relationship with God, following divine commandments, serving others, and preparing for an afterlife. The purpose is often to love God, love your neighbor, and live a righteous life.\\n*   **Eastern Religions (Buddhism, Hinduism):** Meaning is often found in breaking the cycle of rebirth and suffering (samsara). In Buddhism, the goal is to achieve enlightenment (Nirvana) through ethical living, meditation, and wisdom. In Hinduism, it\\'s to understand the true nature of reality (Brahman) and the self (Atman).\\n*   **Taoism:** Meaning is found in living in harmony with the natural flow of the universe, the Tao. It emphasizes simplicity, spontaneity, and balance.\\n\\n### 3. Scientific Perspective\\n\\nScience approaches the question descriptively rather than prescriptively.\\n\\n*   **Biology & Evolution:** From a biological standpoint, the \"purpose\" of life is to survive and reproduce. We are vehicles for our genes, designed to pass them on to the next generation. This doesn\\'t define *personal* meaning but describes the fundamental driving force of all life.\\n*   **Cosmology:** On a cosmic scale, we are a way for the universe to know itself. We are made of stardust, and through our consciousness, the universe experiences beauty, love, art, and inquiry.\\n\\n### 4. Humanistic Perspective\\n\\nThis view focuses on human agency and emphasizes that meaning is found in:\\n\\n*   **Connection and Love:** Forming deep relationships with family, friends, and community.\\n*   **Growth and Fulfillment:** Pursuing knowledge, creativity, and self-actualization (reaching your full potential).\\n*   **Contribution and Legacy:** Making a positive impact on the world, helping others, and leaving something behind that outlives you.\\n\\n### A Practical Synthesis\\n\\nFor many people, the meaning of life isn\\'t one grand answer but a combination of many smaller things. It can be found in:\\n\\n*   **The pursuit of happiness and well-being** for yourself and others.\\n*   **Learning, curiosity, and the experience of wonder.**\\n*   **Creating**—whether it\\'s art, a business, a family, or a garden.\\n*   **Loving** and being loved.\\n*   **Helping** to alleviate the suffering of others.\\n\\n**In short, the most common thread among these answers is that meaning is not something you *find* like a lost object, but something you *build* through your relationships, your work, your passions, and your contributions to the world.**\\n\\nThe question itself is perhaps more important than any answer, as the continual search for meaning is what drives us to live more deeply, ethically, and passionately.', 'refusal': None, 'reasoning': None}}], 'usage': {'prompt_tokens': 11, 'completion_tokens': 965, 'total_tokens': 976, 'prompt_tokens_details': None}}\n"
     ]
    }
   ],
   "source": [
    "# Go up to root directory\n",
    "env_path = Path(os.getcwd()).parents[0] / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Get the API key from environment variable\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Check if API key was loaded\n",
    "if not api_key:\n",
    "    raise Exception(\"OPENROUTER_API_KEY not found in .env file\")\n",
    "\n",
    "# Send the request\n",
    "response = requests.post(\n",
    "    url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    },\n",
    "    data=json.dumps({\n",
    "        \"model\": \"deepseek/deepseek-chat-v3.1:free\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the meaning of life?\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3520a0f4-ff96-48f9-948b-ed53f560704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM call via OpenRouter\n",
    "def llm(prompt, model='deepseek/deepseek-chat-v3.1:free'):\n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"ERROR:\", response.status_code, response.text)\n",
    "        return \"\"\n",
    "\n",
    "    response_json = response.json()\n",
    "    return response_json['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a99759d8-45a6-47ca-9ee1-0d1dbc945c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for chunking\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be3349b0-0ef8-4067-9f1b-3b86b44e43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking logic\n",
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13793776-68e4-44ea-9418-41ee7435359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb552c-2808-4d1b-bb81-c117848153fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the chunks for inspection\n",
    "for chunk in evidently_chunks[:5]:\n",
    "    print(\"Section from:\", chunk['title'])\n",
    "    print(chunk['section'])\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89ef88-ddfa-4cb7-bfe4-319e6ead9f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
